{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data\n",
    "import feature_selection as fs\n",
    "import modeling_2 as model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamegan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/Users/williamegan/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py:747: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/williamegan/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py:688: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = clean_data.runAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'MD_EARN_WNE_P6'\n",
    "df_no_id = df.drop('UNITID', axis=1)\n",
    "n_features_to_select = 20\n",
    "sel1 = fs.F_REGRESSION\n",
    "sel2 = fs.MUTUAL_REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F-SELECTION\n",
    "f_sel_df = fs.selectFeatures(df_no_id, target, n_features_to_select, sel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUTUAL SELECTION\n",
    "mut_sel_df = fs.selectFeatures(df_no_id, target, n_features_to_select, sel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Splitting the Data for Each of the Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling and Splitting for f_regression\n",
    "train, test = model.sampling_data(f_sel_df)\n",
    "x_train, y_train, x_test, y_test = model.split_data(train, test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling and Splitting for mutual_selection\n",
    "train, test = model.sampling_data(mut_sel_df)\n",
    "x_train, y_train, x_test, y_test = model.split_data(train, test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: f_regression features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = 20\n",
    "sel = model.RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_importance, rf, rf_preds = model.run_model(x_train, y_train, x_test, est, sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = .01\n",
    "sel = model.SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm, svr_preds = model.run_model(x_train, y_train, x_test, est, sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = 0\n",
    "sel = model.LINEAR_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr, linear_preds = model.run_model(x_train, y_train, x_test, est, sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = 0\n",
    "sel = model.LASSO_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso, lasso_preds = model.run_model(x_train, y_train, x_test, est, sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run each of these twice and save them each in their own variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this after f_reg\n",
    "f_sel_RF = rf_preds\n",
    "f_sel_SVR = svr_preds\n",
    "f_sel_Lin = linear_preds\n",
    "f_sel_Lass = lasso_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this after mutual_sel\n",
    "mut_sel_RF = rf_preds\n",
    "mut_sel_SVR = svr_preds\n",
    "mut_sel_Lin = linear_preds\n",
    "mut_sel_Lass = lasso_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW, we compare each of these to the truth using MSE and find the smallest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_dict = {}\n",
    "errs_dict['f_sel_RF_err'] =  mean_squared_error(y_test, f_sel_RF ) \n",
    "errs_dict['f_sel_SVR_err'] = mean_squared_error(y_test, f_sel_SVR )\n",
    "errs_dict['f_sel_Lin_err'] = mean_squared_error(y_test, f_sel_Lin )\n",
    "errs_dict['f_sel_Lass_err'] = mean_squared_error(y_test, f_sel_Lass)\n",
    "\n",
    "errs_dict['mut_sel_RF_err'] = mean_squared_error(y_test, mut_sel_RF )\n",
    "errs_dict['mut_sel_SVR_err'] = mean_squared_error(y_test, mut_sel_SVR )\n",
    "errs_dict['mut_sel_Lin_err'] = mean_squared_error(y_test, mut_sel_Lin )\n",
    "errs_dict['mut_sel_Lass_err'] = mean_squared_error(y_test, mut_sel_Lass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = [f_sel_RF_err, f_sel_SVR_err, f_sel_Lin_err, f_sel_Lass_err,  \n",
    "        mut_sel_RF_err, mut_sel_SVR_err, mut_sel_Lin_err, mut_sel_Lass_err]\n",
    "\n",
    "\n",
    "for d in errs:\n",
    "    for i in d\n",
    "\n",
    "\n",
    "winner = min(errs)\n",
    "winner_name = ''\n",
    "\n",
    "for err in errs:\n",
    "    if err == winner:\n",
    "        winner_name\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Tuning (we do this only for our favorite model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we feed it: \n",
    "- the yummy x_train, y_train\n",
    "- the usual target\n",
    "- k to be 5 or 10\n",
    "- cs is a list exponents of 10\n",
    "- error metric is either model.MAE or model.MSE\n",
    "'\n",
    "\n",
    "Once the hungry boi has been fed, it will give us a dictionary with `len(cs)` keys, each containing `k` values. We will avergae the `k` values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Input\n",
    "k=5\n",
    "cs = [10**i for i in range(-2,2)]\n",
    "error_metric = model.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c =  0.01 predicted on the  1 th fold!!\n",
      "c =  0.1 predicted on the  1 th fold!!\n"
     ]
    }
   ],
   "source": [
    "model.xValSVR(x_train, y_train, target, k, cs, error_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get this dictionary, we take the average across the folds for each hyperparameter. The hyperparameter average with the lowest error will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed it:\n",
    "- x_train, y_train,\n",
    "- usual target\n",
    "- est_list, a list of numbers of estimators\n",
    "- error_metric, model.MSE or model.MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Input\n",
    "k=5\n",
    "est_list = [10, 25, 50, 100, 200]\n",
    "error_metric = model.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xValRF(x_train, y_train, target, k, est_list, error_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
